{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Project\n",
    "\n",
    "In this project, you will implement a Deep Averaging Network for sentiment classification.\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "70e6cb858dba42a5eefab2925ff7c3fb",
     "grade": false,
     "grade_id": "cell-18e299b53d44e9c6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "checksum": "cf433779ad9162e4bff9725833fbceb0",
     "grade": false,
     "grade_id": "cell-614a1b162b50f1ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "checksum": "02d3f1caad089b846657d31dc1e401b6",
     "grade": false,
     "grade_id": "cell-8a982dd433cb6b32",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n",
      "GPU is available: True\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "# import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# Set the seed for PyTorch random number generator\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If gpu is supported, then seed the gpu random number generator as well\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    torch.cuda.manual_seed(1)\n",
    "    \n",
    "\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])\n",
    "print (\"GPU is available:\",gpu_available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "f9b076546bc04f0455761e9007c1d7aa",
     "grade": false,
     "grade_id": "cell-69c08fd91b471012",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Deep Averaging Network\n",
    "###  Part One: Load Dataset\n",
    "\n",
    "We use the <a href=https://www.cs.jhu.edu/~mdredze/datasets/sentiment/>multi-domain sentiment</a> dataset created by Professor <a href=https://www.cs.jhu.edu/~mdredze/>Mark Dredze</a> for our project. This dataset contains product reviews taken from Amazon.com from many product types and the reviews are labeled positive and negative. In particular, we only consider the reviews for book for our project. To make things easier for you, we also created a dictionary where you will only consider the words in this dictionary when you are constructing the word embedding for your deep averaging network. Run the following two cells to load the data and see a positive and a negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "checksum": "8459c105e88f5103ef625cd0c195b66e",
     "grade": false,
     "grade_id": "cell-cae72470a03f1d0b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Reviews:  1787\n",
      "Number of Test Reviews:  200\n",
      "Number of Words in the Vocabulary:  4380\n"
     ]
    }
   ],
   "source": [
    "# First load data\n",
    "# Calling load_data return the training review, test review and vocabulary\n",
    "# review_train and review_test are stored as pandas dataframe\n",
    "# vocabulary is dictionary with key-value pairs (word, index)\n",
    "# vocabulary[word] = index\n",
    "# We will use this vocabulary to construct bag-of-word (bow) features\n",
    "review_train, review_test, vocab = load_data()\n",
    "\n",
    "# label 0 == Negative reviews\n",
    "# label 1 == Positive reviews\n",
    "label_meaning = ['Negative', 'Positive']\n",
    "\n",
    "print('Number of Training Reviews: ', review_train.shape[0])\n",
    "print('Number of Test Reviews: ', review_test.shape[0])\n",
    "print('Number of Words in the Vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "checksum": "664fc86c70053a432a47b2f3c672ae91",
     "grade": false,
     "grade_id": "cell-c05892644a77a852",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Positive Training Review:  This was perhaps the best of Johannes Steinhoff's books, since it does not  deal with his own stellar yet tragic WW II and post war career. The  insights of the average person living in Germany are of great importance to  both social and military historians alike. Steinhoff offered this  collective testament as a warning to all of us regarding war and the rise  of a dictator. As Johannes said in an interview, &quot;It is always the  civilians who suffer the most, yet are remembered the least.&quot\n",
      "\n",
      "A Negative Training Review:  I got to page 26 and gave up.  Lockes writings lack focus and are void of humour.  I read as much as I could with patience until it became clear this book was simply someone rambling on about nothing.  Save your money for something worth reading\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print some training review\n",
    "print('A Positive Training Review: ', review_train.iloc[0]['review'])\n",
    "print('A Negative Training Review: ', review_train.iloc[-1]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "c416124aac1636f9c12603ec04562913",
     "grade": false,
     "grade_id": "cell-8209f25427ee055e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We also created a function <code>generate_featurizer</code> takes in a vocabulary and return a bow featurizer based on the vocabulary. Using the returned featurizer, you can convert a sentence into a bag of word feature vector. See the following cell for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "checksum": "41692d4ce7881257cf95cd240e0fa036",
     "grade": false,
     "grade_id": "cell-bf9a446569cfd0bc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple vocabulary\n",
    "simple_vocab = {'learn': 0, 'machine': 1, 'learning': 2, 'teach': 3}\n",
    "\n",
    "# Create a simple sentence that will be converted into bag of words features\n",
    "simple_sentence = ' I learn machine learning to teach machine how to learn.'\n",
    "\n",
    "# Create a featurizer by passing in the vocabulary\n",
    "simple_featurizer = generate_featurizer(simple_vocab)\n",
    "\n",
    "# Call simple_featurizer.transform to transform the sentence to its bag of word features\n",
    "simple_featurizer.transform([simple_sentence]).toarray()\n",
    "\n",
    "# You should get array([[2, 2, 1, 1]]) as output.\n",
    "# This means that the sentence has 2 occurences of 'learn', 2 occurences of 'machine', \n",
    "# 1 occurence of 'learning' and 1 occurence of 'teach'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "f279fbadbe4acfd8ca75375329cca5e3",
     "grade": false,
     "grade_id": "cell-2477750b684a6115",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we will use <code>generate_featurizer</code> to generate a featurizer based on the vocabulary we provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "checksum": "e980b80b1e23bca2e723fcdc422ea516",
     "grade": false,
     "grade_id": "cell-6d837743526c112c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Use the given vocab to generate bag of word featurizer\n",
    "# See the next cell for an example\n",
    "bow_featurizer = generate_featurizer(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "51fc710ef877b704121c77bdb66f8989",
     "grade": false,
     "grade_id": "cell-65b53c8818c65572",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Using the featurizer, we will convert the training reviews and test reviews into bag of word representation and PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "checksum": "352777aea0a97017c95a694072aa3e08",
     "grade": false,
     "grade_id": "cell-2d941e1cd5976a2d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# convert the reviews to bow representation and torch Tensor\n",
    "X_train = torch.Tensor(bow_featurizer.transform(review_train['review'].values).toarray())\n",
    "y_train = torch.LongTensor(review_train['label'].values.flatten())\n",
    "\n",
    "X_test = torch.Tensor(bow_featurizer.transform(review_test['review'].values).toarray())\n",
    "y_test = torch.LongTensor(review_test['label'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "checksum": "18baefed3fb329aa566ad31e5f433e8f",
     "grade": false,
     "grade_id": "cell-4488693f8d463c80",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate PyTorch Datasets \n",
    "trainset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "testset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "# Generate PyTorch Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "ff001e5842de865d673f87ffc2a70054",
     "grade": false,
     "grade_id": "cell-2f35ad9e435c7f99",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Implement a Deep Averaging Network [Graded]\n",
    "\n",
    "We have defined a PyTorch network class for you. What you need to do is to implement the forward pass for your deep averaging network. To start, first implement <code>average</code> that averages the words in a review and then implement <code>forward</code> that passes the \"averaged\" review to a linear layer to produce the model's belief.\n",
    "\n",
    "- For `average` recall that multiplying the matrix of the bag-of-words with the word embeddings will get you the embedded representations for the reviews. You then want to average over all the different words in a review to get an \"average\" embedding for each review. Note that here, we compute a weighted average. I.e. Let $\\mathbf{E}\\in{\\mathcal{R}}^{r\\times v}$ be the embedding matrix (with embedding dimensionality $r$ and vocabulary size $v$). The $i^{th}$ column of $\\mathbf{E}$ is the embedding of word $i$ in the vocabulary (we denote it as $\\mathbf{E}[:,i]$). Further, let $\\mathbf{x}\\in{\\mathcal{R}}^{1\\times v}$ be a horizontal bag-of-words input vector. We compute the average embedding as follows:\n",
    "$$\\mathbf{a}=\\frac{1}{\\sum_i \\mathbf{x}[i]}\\sum_{j}\\mathbf{E}[:,j]\\mathbf{x}[j]$$\n",
    "In the function you need to compute this average for each input vector (the input <code>x</code> is a list of inputs). \n",
    "- For `forward`, pass the output of `average` through the linear layer stored in `self.fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "checksum": "39b082cf3c132a13e09e1ab97cdfdaa4",
     "grade": false,
     "grade_id": "cell-DAN",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a Deep Averaging network model class\n",
    "# embedding_size is the size of the word_embedding we are going to learn\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a word-embedding of dimension embedding_size\n",
    "        # self.embeds is now the matrix E, where each column corresponds to the embedding of a word\n",
    "        self.embeds = torch.nn.Parameter(torch.randn(vocab_size, embedding_size))\n",
    "        self.embeds.requires_grad_(True)       \n",
    "        # add a final linear layer that computes the 2d output from the averaged word embedding\n",
    "        self.fc = nn.Linear(embedding_size, 2) \n",
    "        \n",
    "    def average(self, x):\n",
    "        '''\n",
    "        This function takes in multiple inputs, stored in one tensor x. Each input is a bag of word representation of reviews. \n",
    "        For each review, it retrieves the word embedding of each word in the review and averages them (weighted by the corresponding\n",
    "        entry in x). \n",
    "        \n",
    "        Input: \n",
    "            x: nxd torch Tensor where each row corresponds to bag of word representation of a review\n",
    "        \n",
    "        Output:\n",
    "            n x (embedding_size) torch Tensor for the averaged reivew \n",
    "        '''\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        emb = torch.matmul(x,self.embeds) / torch.sum(x,dim=1, keepdim=True)\n",
    "      \n",
    "\n",
    "    \n",
    "        return emb\n",
    "          \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        This function takes in a bag of word representation of reviews. It calls the self.average to get the\n",
    "        averaged review and pass it through the linear layer to produce the model's belief.\n",
    "        \n",
    "        Input: \n",
    "            x: nxd torch Tensor where each row corresponds to bag of word representation of reviews\n",
    "        \n",
    "        Output:\n",
    "            nx2 torch Tensor that corresponds to model belief of the input. For instance, output[0][0] is\n",
    "            is the model belief that the 1st review is negative\n",
    "        '''\n",
    "        review_averaged = self.average(x)\n",
    "        \n",
    "        out = None\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        ##out = nn.Linear(review_averaged, 2)\n",
    "        out = self.fc(review_averaged)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "checksum": "547524d3fce1cef6c3220942f35fbf52",
     "grade": false,
     "grade_id": "cell-DAN-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: average_test1 ... ✔ Passed!\n",
      "Running Test: average_test2 ... ✔ Passed!\n",
      "Running Test: average_test3 ... ✔ Passed!\n",
      "Running Test: forward_test1 ... ✔ Passed!\n",
      "Running Test: forward_test2 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "def average_test1(): # check the output dinemsions of the average function\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    X = torch.rand(n, vocab_size)    \n",
    "    output_size = model.average(X).shape    \n",
    "    # the output of your forward function should be nx2\n",
    "    return output_size[0] == n and output_size[1] == embedding_size\n",
    "\n",
    "def average_test2():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 3 # vocab size\n",
    "    embedding_size = 5 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # generate a simple input\n",
    "    X = torch.FloatTensor([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 1, 1],\n",
    "    ])\n",
    "    \n",
    "    # Get the averaged reviews\n",
    "    averaged_reviews = model.average(X)\n",
    "    \n",
    "    # Given the input, we know that the first 3 rows corresponds to the first three words\n",
    "    # The last row should be the average of the three words\n",
    "    # The diff between the last row and the average of the first three rows should be small\n",
    "    diff = torch.sum((torch.mean(averaged_reviews[:3], dim=0) - averaged_reviews[3]) ** 2).item()\n",
    "    \n",
    "    return diff < 1e-5\n",
    "\n",
    "def average_test3():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 3 # vocab size\n",
    "    embedding_size = 5 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # generate a simple input\n",
    "    X = torch.FloatTensor([\n",
    "        [1, 1, 1],\n",
    "        [2, 2, 2]\n",
    "    ])\n",
    "    \n",
    "    # Get the averaged reviews\n",
    "    averaged_reviews = model.average(X)\n",
    "    \n",
    "    # Since the 2nd review is a multiple of the first,\n",
    "    # The two averaged review should be the same\n",
    "    diff = torch.sum((averaged_reviews[0] - averaged_reviews[1])**2).item()\n",
    "    \n",
    "    return diff < 1e-5\n",
    "\n",
    "def forward_test1():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    \n",
    "    # call the forward function\n",
    "    X = torch.rand(n, vocab_size)\n",
    "    \n",
    "    output_size = model(X).shape\n",
    "    \n",
    "    # the output of your forward function should be nx2\n",
    "    return output_size[0] == n and output_size[1] == 2\n",
    "\n",
    "def forward_test2():\n",
    "    n = 10 # number of reviews\n",
    "    vocab_size = 5 # vocab size\n",
    "    embedding_size = 32 # embedding size\n",
    "    model = DAN(vocab_size, embedding_size)\n",
    "    X = torch.rand(n, vocab_size)\n",
    "    \n",
    "    logits = model(X) # get the output of your forward pass\n",
    "    \n",
    "    averaged_reviews = model.average(X) # get the intermediate averaged review\n",
    "    logits2 = model.fc(averaged_reviews) # get the model belief using your intermediate average reviews\n",
    "    \n",
    "    return torch.sum((logits - logits2)**2).item() < 1e-5 # Check whether your forward pass is implemented correctly\n",
    "\n",
    "runtest(average_test1, 'average_test1')\n",
    "runtest(average_test2, 'average_test2')\n",
    "runtest(average_test3, 'average_test3')\n",
    "runtest(forward_test1, 'forward_test1')\n",
    "runtest(forward_test2, 'forward_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "checksum": "68fbd60480656b4c49da5acd6066a5e7",
     "grade": true,
     "grade_id": "cell-average_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "checksum": "cf5bf528071bcff37416187396845c6e",
     "grade": true,
     "grade_id": "cell-average_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "checksum": "f63c80aa8f4173b740eed8dd7efad3b6",
     "grade": true,
     "grade_id": "cell-average_test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# average_test3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "checksum": "04991c276ec129b4722fce5798a6befa",
     "grade": true,
     "grade_id": "cell-forward_test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# forward_test1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "checksum": "55304a6ab77487cd60aaa87c276a0665",
     "grade": true,
     "grade_id": "cell-forward_test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder Test Cell\n",
    "# forward_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "checksum": "3b8261b508f3c85f4f20f83fdfe5a367",
     "grade": false,
     "grade_id": "cell-5efa7383fb4c7bb7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = DAN(len(vocab), embedding_size=32)\n",
    "\n",
    "if gpu_available:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "9f3bbba963200a34d3bedf05cc2eb5de",
     "grade": false,
     "grade_id": "cell-f34ae8bcfb938028",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Three: Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "checksum": "50f3d0b2884455afa36f3eaaa788906e",
     "grade": false,
     "grade_id": "cell-06935b2f38a60afb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create optimizer and loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "b62331156b3002d999e2293ed8c0da32",
     "grade": false,
     "grade_id": "cell-756fe2729112ef19",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Four: Train the network\n",
    "\n",
    "Run the following cell to train your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "checksum": "43bd1f03e475d493e8b8847d184084e4",
     "grade": false,
     "grade_id": "cell-3374e25d9a304abf",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100 / 1000] Average Training Accuracy: 0.911058\n",
      "Epoch [100 / 1000] Average Training loss: 0.213666\n",
      "Epoch [200 / 1000] Average Training Accuracy: 0.996995\n",
      "Epoch [200 / 1000] Average Training loss: 0.027080\n",
      "Epoch [300 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [300 / 1000] Average Training loss: 0.007207\n",
      "Epoch [400 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [400 / 1000] Average Training loss: 0.003543\n",
      "Epoch [500 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [500 / 1000] Average Training loss: 0.002242\n",
      "Epoch [600 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [600 / 1000] Average Training loss: 0.001624\n",
      "Epoch [700 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [700 / 1000] Average Training loss: 0.001261\n",
      "Epoch [800 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [800 / 1000] Average Training loss: 0.001019\n",
      "Epoch [900 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [900 / 1000] Average Training loss: 0.000851\n",
      "Epoch [1000 / 1000] Average Training Accuracy: 1.000000\n",
      "Epoch [1000 / 1000] Average Training loss: 0.000729\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "num_epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Define the following variables to keep track of the running losses and accuracies\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for i, (X, y) in enumerate(trainloader):\n",
    "        \n",
    "        # use gpu if necessary\n",
    "        if gpu_available:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        \n",
    "        # clear the gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Do forward propagation to get the model's belief\n",
    "        logits = model(X)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(logits, y)\n",
    "        \n",
    "        # Run a backward propagation to get the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model's parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get the model's prediction \n",
    "        pred = torch.argmax(logits,dim=1)\n",
    "        \n",
    "        # Update the running statistics\n",
    "        running_acc += torch.sum((pred == y).float()).item()\n",
    "        running_loss += loss.item()\n",
    "        count += X.size(0)\n",
    "        \n",
    "    # print the running statistics after training for 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch [{} / {}] Average Training Accuracy: {:4f}'.format(epoch + 1, num_epochs, running_acc / count))\n",
    "        print('Epoch [{} / {}] Average Training loss: {:4f}'.format(epoch + 1, num_epochs, running_loss / len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "63e865f77c6b048cf5f32190fa77bbce",
     "grade": false,
     "grade_id": "cell-d8c31d9dc298f8dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Step 5: Evaluate your model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "checksum": "b1629ccad920bedcdfb40195e9894c3b",
     "grade": false,
     "grade_id": "cell-7a88e5112d7584a3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Test Accuracy is 0.8700\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "# Define the following variable to keep track of the accuracy\n",
    "running_acc = 0.0\n",
    "count = 0.0\n",
    "\n",
    "for (X, y) in testloader:\n",
    "    # Use gpu if available\n",
    "    if gpu_available:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    # Do a forward pass and tell PyTorch that no gradient is necessary to save memory\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "    \n",
    "    # Calculate the prediction\n",
    "    pred = torch.argmax(logits,dim=1)\n",
    "    \n",
    "    # Update the running stats\n",
    "    running_acc += torch.sum((pred == y).float()).item()\n",
    "    count += X.size(0)\n",
    "\n",
    "print('Your Test Accuracy is {:.4f}'. format(running_acc / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "checksum": "4c63090fdb950f424286463b867f93af",
     "grade": false,
     "grade_id": "cell-60dca0660c0ba2c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Run the following cells to see the a random test review and the model prediction. \n",
    "(You may observe that neural networks achieve high accuracy - but tend to be over-confident. This is because they achieve 100% training accuracy early on in the learning procedure and therefore learn that they tend to be always right.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "checksum": "b4d5949ad4f9d1fed23cf7a2279ac330",
     "grade": false,
     "grade_id": "cell-705df6c45ad63992",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:  If you just want the latest research, go read the New England Journal of Medicine.  If you want the latest research PLUS experience, understanding, compassion, and answers to your most embarrassing questions, get this book!  \n",
      "\n",
      "I heard the co-author, Francesca Coltrera, speak a few weeks ago. I was absolutely blown away by how lucid, informed, and compassionate this woman was. She didn't just present the book as a bible for people undergoing breast cancer treatment (though it is that). She talked about it as a a kind of lifeline--a source to go to for questions the doctors and nurses don't have the patience (or experience) to answer.\n",
      "\n",
      "I hope you don't need a book like this.  But if you do, this is the one to get\n",
      "\n",
      "Ground Truth:  Positive\n",
      "Prediction: Negative (Certainty 99.79%)\n"
     ]
    }
   ],
   "source": [
    "target = torch.randint(high=len(testset), size=(1,)).item()\n",
    "\n",
    "review_target, label_target = review_test.iloc[target]\n",
    "if gpu_available:\n",
    "    bog_target = testset[target][0].unsqueeze(0).cuda()\n",
    "else:\n",
    "    bog_target = testset[target][0].unsqueeze(0)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_target = model(bog_target)\n",
    "\n",
    "pred = torch.argmax(logits_target, dim=1)\n",
    "probability=torch.exp(logits_target.squeeze())/torch.sum(torch.exp(logits_target.squeeze()))\n",
    "\n",
    "print('Review: ', review_target)\n",
    "print('Ground Truth: ', label_meaning[int(label_target)])\n",
    "print('Prediction: %s (Certainty %2.2f%%)' % (label_meaning[pred.item()],100.0*probability[pred.item()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
